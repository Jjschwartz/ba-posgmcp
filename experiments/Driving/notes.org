#+TITLE: BAPOSGMCP Driving Environment Notes

* Memory Usage

Memory usage is affected by:

- Network size
- Batch size
- Optimizer (since GPU stores optimizer state)
- Sequence Length

And probably other things as well, but these are the main ones (I think).

** Batch size vs memory usage

Using the default network size =[64, 32]= with =256= LSTM units.

*Summary*:

- GPU memory usage increases with both =sgd_minibatch_size= and =train_batch_size=.
- Need to monitor =nvtop= output to get actual memory usage, output from rllib tells you nothing since it is only a sample (?)
- Memory usage seems to scale super linearly with batch size, but increases are not too drastic. E.g. increasing =train_batch_size= from =256= to =8192= lead to an increase of =127MB= and =114MB= in Host and GPU memory usage respectively which is only around =1%= of an =8GB= GPU's RAM.

  *Note* =Host Mem= is the =Resident Memory= of the host (i.e. the CPU RAM being used by the process) while =GPU Mem= is the GPU RAM being used. What we are really interested in is the =GPU Mem= and =GPU= measure since these determine how much of the GPU is being used and ultimately the size and number of networks we can run at the same time.

*** Testing effect of =sgd_minibatch_size=

| train_batch_size | sgd_minibatch_size |  CPU | CPU Ram |  GPU | GPU Ram | Host Mem | GPU Mem      |
|------------------+--------------------+------+---------+------+---------+----------+--------------|
|             2048 |                 64 | 14.8 |   71.19 | 32.1 |    20.0 |          |              |
|             2048 |                128 | 14.9 |   71.28 | 28.0 |    20.0 |          |              |
|             2048 |                256 | 15.4 |    71.6 | 28.8 |    20.3 | 2804 MB  | 1041MB (13%) |
|             2048 |                512 | 14.7 |    71.6 | 22.3 |    20.3 |          |              |
|             2048 |               1024 | 14.8 |    71.9 | 19.3 |    20.5 | 2808 MB  |              |
|             2048 |               2048 | 14.8 |    72.0 | 13.0 |    20.2 | 2806 MB  | 1085MB (14%) |

*** Testing effect of =train_batch_size=

| train_batch_size | sgd_minibatch_size |  CPU | CPU Ram |  GPU | GPU ram | Host Mem | GPU Mem      |
|------------------+--------------------+------+---------+------+---------+----------+--------------|
|              256 |                256 | 17.5 |    72.1 | 27.0 |    19.2 | 2783MB   | 1005MB (13%) |
|              512 |                256 | 16.5 |    72.1 | 22.0 |    19.2 | 2786MB   |              |
|             1024 |                256 | 14.3 |    72.4 | 22.5 |    19.3 | 2789MB   |              |
|             2048 |                256 | 14.8 |    72.7 | 20.0 |    19.5 | 2804MB   | 1041MB (13%) |
|             4096 |                256 | 14.7 |    73.1 | 22.8 |    19.8 | 2838MB   |              |
|             8192 |                256 | 15.5 |    74.2 | 23.2 |    20.6 | 2910MB   | 1119MB (14%) |

** Network size vs memory usage

Here we test network size vs memory usage. We use the default =train_batch_size=2048= and =sgd_batch_size=256= and =256= LSTM units.

*Summary*

- As expected GPU memory usage increases with network size, both Fully connected network
- Scaling is not too drastic though, at least for the small networks I am using.

| Network size    | LSTM |  CPU | CPU Ram |  GPU | GPU Ram | Host Mem | GPU Mem      |
|-----------------+------+------+---------+------+---------+----------+--------------|
| [64, 32]        |  256 | 11.5 |    72.1 | 27.0 |    21.1 | 2804MB   | 1039MB (13%) |
| [256, 256]      |  256 | 13.2 |    72.5 | 18.8 |    21.2 | 2823MB   | 1047MB (13%) |
| [512, 512]      |  256 | 11.7 |    73.0 | 26.3 |    20.0 | 2838MB   | 1069MB (13%) |
| [64, 32]        |  512 | 11.5 |    73.5 | 28.0 |    20.2 | 2839MB   | 1081MB (14%) |
| [256, 256, 256] |  256 | 13.5 |    73.8 | 26.7 |    19.8 | 2814MB   | 1051MB (13%) |
| [256]           |  256 | 11.5 |    74.1 | 20.0 |    19.7 | 2807MB   | 1045MB (13%) |
|                 |      |      |         |      |         |          |              |

* Solving issue with performance slowing down over time - solved :)

From the  experiment logs I can see that the average time per episode increases over time even though the number of steps per episode remains the same and the policies shouldn't be affected (in terms of episode time) by the experiment parameters. The general trend is that performance is fast (< 1 sec per episode) and then at some point, after a certain number of experiments or time has passed, performance slows significantly. At it's worst I observed episodes taking 500+ seconds!

After viewing =htop= during a run where the episode times had slowed significantly my hypothesis is that for each experiment run =ray= is spawning processes and threads and these are not being cleaned up. Overtime this leads to multiple threads/processes competing for limited resources, eventually leading to significant slowing of the main processes actually running the experiments.

** Before the fix

Running command:

#+begin_src shell

  python pairwise_comparison.py --env_names Driving7x7CrissCross1-v0  Driving7x7CrissCross2-v0 Driving7x7CrissCross3-v0 Driving7x7CrissCross4-v0 --policy_dirs rl_policies/Driving7x7CrissCross1-v0_2022-05-12\ 13\:47\:17.024979/ rl_policies/Driving7x7CrissCross1-v0_2022-05-13\ 10\:12\:22.733385/ rl_policies/Driving7x7CrissCross2-v0_2022-05-13\ 00\:13\:44.494543/ rl_policies/Driving7x7CrissCross2-v0_2022-05-13\ 10\:12\:13.059530/ --num_episodes 1000 --seed 0 --n_procs 3

#+end_src

*** Observations from log files

- *Run started*: 21:07:09
- *Exp 10*
  - Start time = 21:08:59
  - pi_1 vs pi_1
  - End time = 21:09:16
  - Ep time mean = 0.016  +/- 0.005
  - Ep steps mean = 6.88 +/- 1.55
  - Time per step = 0.0023
- *Exp 77*
  - Start time = 21:33:01
  - pi_1 vs pi_1
  - End time = 21:36:15
  - Ep time mean = 0.19 +/- 0.19
  - Ep steps mean = 17.09 +/- 16.94
  - Time per step = 0.011
- *Exp 97*
  - Start time = 22:05:49
  - pi_1 vs pi_1
  - End time = 22:12:06
  - Ep time mean = 0.373 +/- 0.416
  - Ep steps mean = 11.29 +/- 11.98
  - Time per step = 0.033

    There is a more drastic increase in episode time starting from around experiment 100, with episode time increasing from 0.5 s to upto 4 s for exps 100-105, then in exp 106 times shoot up to 23.44 +/- 30.11 (steps = 11.56 +/- 11/41).

    Exp 106 took a total of around 6.5 hours to run.

    Times were lower for exp 107 (5.8 +/- 7.06 s per episode), but then increased again for exp 108 and 109 which had many episodes lasting 90 sec +.

*** Other observations

Issue is present if running experiments sequentially, and also if running 1000, 100, or 10 episodes.

** Fix attempt 1 - reduce logging from ray - faile

I noticed in =htop= a lot of processes spawned by =ray= are to do with logging and a dashboard. I also notice when killing a slowed down experiment run a lot of messages relating to =logging.cc= being killed.

My hypothesis is that for each experiment a new rllib trainer is loaded, in order to load the policy, which in turn loads a bunch of extra processes that are not cleaned up until all experiments are finished.

From the documentation it's possible to control ray tuning using arguments passed to the =ray.init= function.

ref: https://docs.ray.io/en/latest/ray-core/package-ref.html

*Outcome* = fail

In this fix I set the following arguments for the =ray.init= function:

- =include_dashboard=False=
- =log_to_driver=False=

This lead to the program freezing :( I think it may be due to trying to spawn multiple processes within a multi-process program (i.e. after ray.init() is called)

** Fix attempt 2 - adding =trainer.cleanup()= call after importing policy - fail-

*Outcome* = fail

The hope here is that any unused processes created when importing a policy get cleaned up.

*** Observations from =htop=

I ran the command and noticed that on htop the =load average= gradually increased overtime with the number of experiments run. This was true when the number of episodes being run was =1000=, =100=, or =10= providing some support for the hypothesis that the slowdown is due to the extra processes spawned per experiment.
** Fix attempt 3 - calling =ray.shutdown= then =ray.init= within each new experiment process - fail

I think each call to a new experiment was adding more ray processes without cleaning up the old ones. By calling =ray.shutdown= within each each new process it closes all ray resources spawned by the previous experiment within the same process before starting the new experiment.

This doesn't seem to have solved the issue :/

** Fix attempt 4 - setting ="log_sys_usage=False"= in the PPOTrainer config - worked

By default each trainer uses the =ray.tune.utils.UtilMonitor= to monitor CPU, GPU, VRAM, RAM usage. This involves creating a new thread which queries the OS every x seconds to get the latest usage figures. Since in my experiments I am creating a new trainer each experiment, that means each time a new trainer is created there is an additional thread that is periodically querying the OS. This slowly builds up until the OS has to keep interupting running processes in order to respond to these queries, and the issue only gets worse as the number of experiments and hence trainers grows.

Setting ="log_sys_usage=False"= in the trainer config solves this problem. It can also be solved by running the =trainer.stop()= function which releases all resources used by the trainer. Note, however using the first solution greatly reduces the overhead each time a new trainer is created since there is no waiting for the sys_usage monitor thread to end - i.e. 210 sec with =true= vs 63 sec with =false= for 268 experiments each with 10 episodes.

I added cleanup of trainers to the =baposgmcp.rllib.import_rllib_policy= and =baposgmcp.rllib.import_igraph_policies=, so hopefully this issue won't be a thing in the future

* Solving issue with some  experiments not running when using slurm

*Problem* When running =pairwise_comparison.py= and =experiment.py= on the sococluster using slurm, when running a Job Array using 16 cores and 62 GB per job. I am encountering the problem where some experiments fail to run for some reason, instead they just hang and are only terminated once the slurm time limit is reached.

*Update* I thing my problem may be related to this [[https://github.com/ray-project/ray/issues/21479][issue]]. Basically, the worker is taking to long to load which leads to a timeout and exiting of the program. It's interesting that this doesn't cause my entire program to crash, or raise an error within the program. Maybe this is because the whole process the experiment run is on crashes?

A thing about this bug is that it is somewhat random, since load times can be different each run. That means some runs will have no issues while others do.

*Update 2* After looking at the =exp_X.log= and slurm log files, the problem seems to happen for KLR policies. Based on the result dir creation times, it looks like the KLR policies take longer to generate than the SP policies (which makes sense). It could be due to the KLR policies requesting too many resources, for example if each policy loaded requires a single CPU.

*Update 3* After looking at =htop= while running experiments, it's clear that there is a huge surge in CPU usage (both % and number) when loading an experiment. This makes sense since we are loading a group of policies, each of which is a Rllib trainer which consumes a CPU.

** Summary of failed =experiment.py= run

For =num_sims=8=

=Failures= - determined by looking at number of =exp_X.log= files created. There should only be 16, given the experiment was run using 16 cores and only for a very brief time.

=Errors= - determined by looking for number of =core_worker.cc:137: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory= error messages in the slurm log file.

*** SP - Seed 0
- =experiment_numsims_8_2022-06-07_02-19-55dludpv6i=
- Failures - none apparent
- =2136_5=
- Errors - 0
*** SP - Seed 1
- =experiment_numsims_8_2022-06-07_02-19-55lo8h3aw1=
- Failures - none apparent
- =2136_6=
- Errors - 0
*** SP - Seed 2
- =experiment_numsims_8_2022-06-07_02-19-55ltvszu2o=
- Failures - None apparent
- =2136_7=
- Errors - 0
*** SP - Seed 3 (not run)
*** SP - Seed 4 (not run)
*** KLR - Seed 0
- =experiment_numsims_8_2022-06-07_02-19-58uckm90ez=
- Failures - 24
- =2136_0=
- Error - 26
*** KLR - Seed 1
- =experiment_numsims_8_2022-06-07_02-19-56a525frge=
- Failures - 4
- =2136_1=
- Error - 4
*** KLR - Seed 2
- =experiment_numsims_8_2022-06-07_02-19-57oztt7ywn=
- Failures - 2
- =2136_2=
- Errors - 2
*** KLR - Seed 3
- =experiment_numsims_8_2022-06-07_02-19-56s1l8xu8e=
- Failures - 15
- =2136_3=
- Errors - 15
*** KLR- Seed 4
- =experiment_numsims_8_2022-06-07_02-19-55p0mk01dz=
- Failures - 1
- =2136_4=
- Errors - 0

  Failure (in file creation) probably due to process of terminating job.




** DONE Fix attempt 1 - include =ray.init(num_cpus=n_procs, include_dashboard=False)= in main function - failed
CLOSED: [2022-06-07 Tue 11:47]

[[https://stackoverflow.com/questions/72464756/ray-on-slurm-problems-with-initialization][reference]]

The belief here is that ray is trying to assign more CPUs than it's allowed. This results in some processes being killed by slurm.

This fix failed to work as it lead the program hanging. I think the hanging is due to the interaction between =ray= and python multiprocessing. I saw a similar issue when trying to debug the slow down problem.

** DONE Fake Fix  - it weirdly started working when I set the "log_level" of rllib policies to be "DEBUG"
CLOSED: [2022-06-07 Tue 11:47]

I have no idea why this seems to have fixed the issue :/

*Update* I don't think this did fix the issue. I think I just had a lucky run.

** DONE Fix attempt 2 - make it so no rollout workers are registered when loading policies
CLOSED: [2022-06-07 Tue 11:48]

Rollout workers are not needed for the experiments since we just want the policies, so I am going to play with the different trainer configuration to try load a policy without loading a rollout worker.

*Update* I played around with different settings and they didn't seem to have an effect. I think (without diving deep into the source code) it is hard to stop a Trainer generating a rollout worker. Also I believe that by setting =num_workers=0= in the config, any rollout worker is also on the main trainer worker, so it shouldn't impact the num CPUS. I'm pretty sure the problem is =ray= using more CPUs that it should when creating the trainers. See next heading for follow-up.

** Fix attempt 1 (redux) - include =ray.init(num_cpus=nprocs)= - [FIXED]

When running on my local machine with =n_procs=1= this does have a significant impact on the number of CPUs deployed. Specifically, when I use =ray.init(num_cpus=1)= only a single =ray::IDLE= process is generated (as observed on =htop=) compared to around 8 processes when I didn't include it. This suggests to me that without specifying =ray.init= =ray= is requesting multiple CPUs rather than the single CPU that I want.

The tricky part is how do I get it to work with python multiprocessing :/ Maybe the solution is to move experiments to running as =ray.actors=.

Before that let's try a few things (using =n_procs=2=):

*** Simplest naive solution - Adding =ray.init(num_cpus=nprocs)= in main function (before python multiprocessing is called)

*Result* both processes hang while loading ray policies from file. When I interupt the program with =Ctrl-C= the error raised is within the python threading library, suggesting there is some funky interaction between ray and python multiprocessing, as expected.

*** Adding =ray.init(num_cpus=1)= in =exp.run_single_experiment= function (the function called by python multiprocessing)

*Result* both processes ran for the first experiment and also only generated a single =ray::IDLE= process each. However, they crashed upon beginning the second experiment (for each given process) due to calling =ray.init= twice.

*Update* I fixed the issue re calling =ray.init= twice by calling =ray.init(num_cpus=1)= within the =initializer= function that can be passed to =mp.Pool=. This seemed to do the trick. The program runs and only a single =ray::IDLE= process is generated per experiment.

*Bonus* calling =ray.init= also allows me to specify =include_dashboard=False= which gets rid of that annoying background process.
