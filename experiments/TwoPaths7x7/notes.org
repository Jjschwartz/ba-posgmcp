#+TITLE: Notes for BA-POSGMCP for the TwoPaths7x7-v0 environment

* Description

The *purpose* of this experiment is to verify that BA-POSGMCP works as expected.

*How?* In this experiment the aim is to check:

1. Training of the RL policies actually learns something and it converges.
2. The converged RL policies get the expected performance. That is the trained policies achieve near optimal returns against opponents one level down in the hiearchy, and bad performance against the opponent directly above them in the hiearchy.
3. BA-POSGMCP learns the correct distribution over the opponent policies and it's performance converges to near optimal for an unchanging opponent.

* Experiment Parameters

** Environment

Experiments will be run on the =TwoPaths7x7-v0= environment using deterministic actions and infinite horizon for 100 steps per episode. The infinite horizon means that agents are reset to the start position when a terminal state is reached, rather than the episode ending. This setting is used so that it give BA-POSGMCP time to learn and adapt to the opponents policy within a single episode. Without using automatic resetting adaptation would not be possible for the TwoPaths environment since the only time the opponent is potentially observed is when the episode ends.

** RL Policies

We will use a reasoning level =K= of =K=3= since this more than captures all the different possible policies under the KLR setup in the TwoPaths environment, since there are only two paths and hence two possible strategies given the opponent is following a stationary policy.

* Experiment procedure

** RL policy training

Policies where trained using the =SyKLR= algorithm. We used the =Rllib= implementation of the =PPO= algorithm with an =LSTM= network to handle partial observability. Policies were synchronized every =2000= training steps, and we trained for a total of =400,000= steps.

* Results

** RL Policies training

Policies to somewhat converged to the expected performance relationship, although there was some instability in the training.

[[./figures/rl_policies_training_curves.png]]

** RL Policy Pairwise Performance

We see the expected relationships somewhat reflected in the pairwise comparison, where performance is always good when the agent has a reasoning level one higher than the opponent. There is some deviation from the other expected relationships. This is primarily due to the level 0 chaser policy taking the left hand, longer, path than the expected right hand, shorter, path. This difference in choice is sub-optimal but only by a very small amount given the discount horizon (i.e. for for discount=0.95 it would be like a difference of maybe 0.1-0.5 in the expected return, maybe even less).

*Note* the plot shows performance for the row (runner) agent against the column (chaser) agent with indicated reasoning levels for each agent.

[[./figures/pw_rl_performance.png]]

** BAPOSGMCP Performance

We see a more even performance across the different opponent reasoning levels. The performance isn't great but this is performance using =128= sims per step. I will need to run it with more and see how performance scales with compute.

[[./figures/pw_baposgmcp_performance.png]]
