#+TITLE: Driving PPO Tuning Notes

* Defaults

#+begin_src python

{
  "gamma": 0.99,
  "use_critic": True,
  "use_gae": True,
  "lambda": 1.0,
  "kl_coeff": 0.2,
  "rollout_fragment_length": 100,
  "train_batch_size": 2048,
  "sgd_minibatch_size": 256,
  "shuffle_sequences": True,
  "num_sgd_iter": 10,
  "lr": 0.0003,
  "lr_schedule": None,
  "vf_loss_coeff": 1.0,
  "model": {
      # === Model Config ===
      # ref: ray-project/ray/blob/releases/1.12.0/rllib/models/catalog.py
      # === Built-in options ===
      # FullyConnectedNetwork (tf and torch): rllib.models.tf|torch.fcnet.py
      # These are used if no custom model is specified and the input space is
      # 1D. Number of hidden layers to be used.
      "fcnet_hiddens": [256, 256],
      # Activation function descriptor.
      # Supported values are: "tanh", "relu", "swish" (or "silu"),
      # "linear" (or None).
      "fcnet_activation": "tanh",

      # Whether layers should be shared for the value function.
      "vf_share_layers": False,

      # == LSTM ==
      # Whether to wrap the model with an LSTM.
      "use_lstm": True,
      # Max seq len for training the LSTM, defaults to 20.
      "max_seq_len": 20,
      # Size of the LSTM cell.
      "lstm_cell_size": 256,
      # Whether to feed a_{t-1} to LSTM (one-hot encoded if discrete).
      "lstm_use_prev_action": False,
      # Whether to feed r_{t-1} to LSTM.
      "lstm_use_prev_reward": False,
  },
  "entropy_coeff": 0.0,
  "entropy_coeff_schedule": None,
  "clip_param": 0.3,
  "vf_clip_param": 15.0,
  "grad_clip": None,
  "kl_target": 0.01,
  # "trancate_episodes" or "complete_episodes"
  "batch_mode": "truncate_episodes",
  "optimizer": {},
}

#+end_src

* Experiment settings

#+begin_src python

  {
    # A single rollout worker with a single env
    "num_workers": 1,
    "num_envs_per_worker": 1,
  }

#+end_src


* fcnetwork sizes:

Performance is similar between big and small networks tested (except 1 seed for small network) however small network was faster, requiring only 25 min to train as opposed to 30 min for the bigger network (due to faster learn and inference time). There was no noticeable difference in CPU memory usage between the different network sizes.

** 256, 256

[[./figures/256x256_timesteps.png]]

[[./figures/256x256_time.png]]

** 64, 32

[[./figures/64x32_timesteps.png]]

[[./figures/64x32_time.png]]

* Exploration
**
