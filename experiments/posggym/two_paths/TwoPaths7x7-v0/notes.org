#+TITLE: Notes for BA-POSGMCP for the TwoPaths7x7-v0 environment

* Description

The *purpose* of this experiment is to verify that BA-POSGMCP works as expected.

*How?* In this experiment the aim is to check:

1. Training of the RL policies actually learns something and it converges.
2. The converged RL policies get the expected performance. That is the trained policies achieve near optimal returns against opponents one level down in the hiearchy, and bad performance against the opponent directly above them in the hiearchy.
3. BA-POSGMCP learns the correct distribution over the opponent policies and it's performance converges to near optimal for an unchanging opponent.

* Experiment Parameters

** Environment

Experiments will be run on the =TwoPaths7x7-v0= environment using deterministic actions and infinite horizon for 100 steps per episode. The infinite horizon means that agents are reset to the start position when a terminal state is reached, rather than the episode ending. This setting is used so that it give BA-POSGMCP time to learn and adapt to the opponents policy within a single episode. Without using automatic resetting adaptation would not be possible for the TwoPaths environment since the only time the opponent is potentially observed is when the episode ends.

** RL Policies

We will use a reasoning level =K= of =K=3= since this more than captures all the different possible policies under the KLR setup in the TwoPaths environment, since there are only two paths and hence two possible strategies given the opponent is following a stationary policy.
